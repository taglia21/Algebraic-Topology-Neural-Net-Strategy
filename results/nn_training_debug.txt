NN Training Diagnostics - 2026-01-13T17:36:45.362044
============================================================


════════════════════════════════════════════════════════════
NN TRAINING DIAGNOSTICS
════════════════════════════════════════════════════════════

[1] TRAINING DATA:
    X_train shape: (215, 15, 6)
    X_train dtype: float32
    X_train mean:  0.015942
    X_train std:   1.012850
    X_train min:   -3.519399
    X_train max:   4.471611
    X_train NaN:   0
    X_train Inf:   0

    y_train shape: (215,)
    y_train dtype: float32
    y_train mean:  0.4419 (should be ~0.5 for balanced)
    y_train sum:   95.0 (class 1 count)
    y_train class balance: 44.2% up, 55.8% down

[2] TRAINING HISTORY:
    Epochs completed: 48
    Initial loss:    0.691887
    Final loss:      0.559353
    Loss reduction:  0.132534
    ✓ Loss decreased during training
    Initial val_loss: 0.691552
    Final val_loss:   0.669954
    Initial accuracy: 51.16%
    Final accuracy:   72.67%
    Final val_acc:    55.81%

[3] MODEL WEIGHTS (Post-Training):
    Total params:    31137
    Weights mean:    0.003623
    Weights std:     0.104334
    Weights min:     -0.608248
    Weights max:     1.065535
    Zero weights:    0 (0.0%)
    LSTM weight sample: [-0.003073, 0.125231, -0.083927, 0.075856, 0.005757, -0.000679, -0.029432, 0.098667, -0.140642, -0.166942]
    ✓ Weights have reasonable variance

[4] MODEL CHECKPOINT:
    Path: /workspaces/Algebraic-Topology-Neural-Net-Strategy/results/model_weights.weights.h5
    Size: 0.39 MB (405,560 bytes)
    ✓ Checkpoint file size looks reasonable

[5] POST-TRAINING INFERENCE TEST:
    10 random inputs → outputs:
      Test 1: 0.563077
      Test 2: 0.610910
      Test 3: 0.384682
      Test 4: 0.508286
      Test 5: 0.716228
      Test 6: 0.523434
      Test 7: 0.341573
      Test 8: 0.480173
      Test 9: 0.505068
      Test 10: 0.420538

    Output mean: 0.505397
    Output std:  0.104159
    ✓ Outputs vary across different inputs (model is trained)

[6] INFERENCE ON ACTUAL TRAINING DATA:
    20 training samples → outputs:
      Sample   0: output=0.3987, actual=1, pred=0 ✗
      Sample  11: output=0.3524, actual=0, pred=0 ✓
      Sample  22: output=0.7585, actual=1, pred=1 ✓
      Sample  33: output=0.3485, actual=0, pred=0 ✓
      Sample  45: output=0.6514, actual=0, pred=1 ✗
      Sample  56: output=0.4541, actual=0, pred=0 ✓
      Sample  67: output=0.5209, actual=0, pred=1 ✗
      Sample  78: output=0.5152, actual=0, pred=1 ✗
      Sample  90: output=0.6749, actual=1, pred=1 ✓
      Sample 101: output=0.4439, actual=1, pred=0 ✗
      Sample 112: output=0.3607, actual=0, pred=0 ✓
      Sample 123: output=0.3230, actual=0, pred=0 ✓
      Sample 135: output=0.5016, actual=1, pred=1 ✓
      Sample 146: output=0.2850, actual=0, pred=0 ✓
      Sample 157: output=0.1662, actual=0, pred=0 ✓
      Sample 168: output=0.5319, actual=1, pred=1 ✓
      Sample 180: output=0.4320, actual=1, pred=0 ✗
      Sample 191: output=0.3625, actual=0, pred=0 ✓
      Sample 202: output=0.4239, actual=1, pred=0 ✗
      Sample 214: output=0.5024, actual=0, pred=1 ✗

    Training data output mean: 0.450382
    Training data output std:  0.136144

════════════════════════════════════════════════════════════
DIAGNOSTIC VERDICT:
════════════════════════════════════════════════════════════
✓ No obvious issues detected. Model appears trained.
  If outputs are still ~0.5 during backtest, check:
  1. Is the correct model being loaded?
  2. Are features computed the same way during inference?
════════════════════════════════════════════════════════════

Full diagnostics saved to: /workspaces/Algebraic-Topology-Neural-Net-Strategy/results/nn_training_debug.txt
